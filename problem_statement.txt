Spark Version:spark-3.0.0-bin-hadoop3.2

Cassandra Details:
cassandra Host: 13.232.25.194
cassandra Username: cassandra
cassandra Password: cassandra
keyspace: poc
table: dev_event

CREATE TABLE poc.dev_event (
    dev_id text,
    event int,
    firstOccurrenceTime timestamp,
    lastOccurrenceTime timestamp,
    occurrenceCount int,
    receiveTime timestamp,
    persistTime timestamp,
    state text,
    PRIMARY KEY (dev_id, firstOccurrenceTime, lastOccurrenceTime)
);

Code: 
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType
from pyspark.sql.functions import from_json, col
from pyspark.sql import SparkSession

# Create Spark session
spark = SparkSession \
  .builder \
  .appName("KafkaStreamToDataFrame") \
  .config("spark.cassandra.connection.host", "13.232.25.194") \
  .config("spark.cassandra.auth.username", "cassandra") \
  .config("spark.cassandra.auth.password", "cassandra") \
  .getOrCreate()

# Set the Spark log level to ERROR
sc = spark.sparkContext
sc.setLogLevel('ERROR')

# Define the Kafka topic name and host
topic_name = "ext_device-event_10121"
kafka_host = "zonos.engrid.in:9092"

# Read the data from the Kafka topic into a DataFrame
df = spark \
  .readStream \
  .format("kafka") \
  .option("kafka.bootstrap.servers", kafka_host) \
  .option("subscribe", topic_name) \
  .option("startingOffsets", "earliest") \
  .load()

# Print the schema of the DataFrame
df.printSchema()

# Define the schema for the data
schema = StructType([
  StructField("device", StringType(), True),
  StructField("event", StringType(), True),
  StructField("firstOccurrenceTime", TimestampType(), True),
  StructField("lastOccurrenceTime", TimestampType(), True),
  StructField("occurrenceCount", IntegerType(), True),
  StructField("receiveTime", TimestampType(), True),
  StructField("persistTime", TimestampType(), True),
  StructField("state", StringType(), True),
  StructField("context", StringType(), True)
])

# Parse the value column into a DataFrame using the defined schema
parsed_df = df.selectExpr("CAST(value AS STRING)") \
  .select(from_json(col("value"), schema).alias("data")) \
  .select("data.*")

# Write the parsed DataFrame to Cassandra using foreachBatch
def write_to_cassandra(batch_df, batch_id):
    if batch_df is not None:
        batch_df.write \
          .format("org.apache.spark.sql.cassandra") \
          .options(table="dev_event", keyspace="poc") \
          .mode("append") \
          .option("confirm.truncate", "true") \
          .option("spark.cassandra.output.ignoreNulls", "true") \
          .option("spark.cassandra.output.batch.grouping.buffer.size", "5000") \
          .option("spark.cassandra.output.concurrent.writes", "100") \
          .option("spark.cassandra.output.batch.grouping.key", "device") \
          .save()

query = parsed_df \
  .writeStream \
  .foreachBatch(write_to_cassandra) \
  .start()

query.awaitTermination()


Output:
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)

23/02/16 06:31:36 ERROR MicroBatchExecution: Query [id = e9648e0d-baa7-4980-9bb8-d26b9aca8549, runId = eb98c158-531c-44df-a6a4-5e8029e5645d] terminated with error
py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "/mnt/spark/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 2442, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/mnt/spark/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/utils.py", line 210, in call
    raise e
  File "/mnt/spark/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/utils.py", line 207, in call
    self.func(DataFrame(jdf, self.sql_ctx), batch_id)
  File "/home/avs/spark_project/device_event_to_cassandra/dev_ev_to_cassandra.py", line 64, in write_to_cassandra
    .save()
  File "/mnt/spark/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 825, in save
    self._jwrite.save()
  File "/mnt/spark/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
    return_value = get_return_value(
  File "/mnt/spark/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/utils.py", line 137, in deco
    raise_from(converted)
  File "<string>", line 3, in raise_from
pyspark.sql.utils.IllegalArgumentException: Invalid batch level: device

	at py4j.Protocol.getReturnValue(Protocol.java:476)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)
	at com.sun.proxy.$Proxy17.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:56)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:56)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:36)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:573)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:571)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:571)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:223)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)
	at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)
Traceback (most recent call last):
  File "/home/avs/spark_project/device_event_to_cassandra/dev_ev_to_cassandra.py", line 71, in <module>
    query.awaitTermination()
  File "/mnt/spark/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/streaming.py", line 103, in awaitTermination
  File "/mnt/spark/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
  File "/mnt/spark/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/utils.py", line 137, in deco
  File "<string>", line 3, in raise_from
pyspark.sql.utils.StreamingQueryException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "/mnt/spark/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 2442, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File "/mnt/spark/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/utils.py", line 210, in call
    raise e
  File "/mnt/spark/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/utils.py", line 207, in call
    self.func(DataFrame(jdf, self.sql_ctx), batch_id)
  File "/home/avs/spark_project/device_event_to_cassandra/dev_ev_to_cassandra.py", line 64, in write_to_cassandra
    .save()
  File "/mnt/spark/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 825, in save
    self._jwrite.save()
  File "/mnt/spark/spark-3.0.0-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py", line 1304, in __call__
    return_value = get_return_value(
  File "/mnt/spark/spark-3.0.0-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/sql/utils.py", line 137, in deco
    raise_from(converted)
  File "<string>", line 3, in raise_from
pyspark.sql.utils.IllegalArgumentException: Invalid batch level: device

=== Streaming Query ===
Identifier: [id = e9648e0d-baa7-4980-9bb8-d26b9aca8549, runId = eb98c158-531c-44df-a6a4-5e8029e5645d]
Current Committed Offsets: {}
Current Available Offsets: {KafkaV2[Subscribe[ext_device-event_10121]]: {"ext_device-event_10121":{"23":3971,"32":2380,"35":10420,"8":5055,"17":3372,"26":3790,"11":3020,"29":3620,"2":6272,"20":5050,"5":3265,"14":3916,"4":3109,"13":5350,"31":4108,"22":2558,"7":3432,"16":2205,"34":2569,"25":2495,"10":2535,"1":2466,"28":10385,"19":3997,"27":2802,"9":2933,"18":2264,"12":4817,"3":4463,"21":3630,"30":2884,"15":3160,"33":4574,"6":3626,"24":3423,"0":9795}}}

Current State: ACTIVE
Thread State: RUNNABLE

Logical Plan:
Project [data#23.device AS device#25, data#23.event AS event#26, data#23.firstOccurrenceTime AS firstOccurrenceTime#27, data#23.lastOccurrenceTime AS lastOccurrenceTime#28, data#23.occurrenceCount AS occurrenceCount#29, data#23.receiveTime AS receiveTime#30, data#23.persistTime AS persistTime#31, data#23.state AS state#32, data#23.context AS context#33]
+- Project [from_json(StructField(device,StringType,true), StructField(event,StringType,true), StructField(firstOccurrenceTime,TimestampType,true), StructField(lastOccurrenceTime,TimestampType,true), StructField(occurrenceCount,IntegerType,true), StructField(receiveTime,TimestampType,true), StructField(persistTime,TimestampType,true), StructField(state,StringType,true), StructField(context,StringType,true), value#21, Some(Asia/Kolkata)) AS data#23]
   +- Project [cast(value#8 as string) AS value#21]
      +- StreamingDataSourceV2Relation [key#7, value#8, topic#9, partition#10, offset#11L, timestamp#12, timestampType#13], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@6b8785fb, KafkaV2[Subscribe[ext_device-event_10121]]


Fix the error for me please in the code